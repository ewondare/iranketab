{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy_utils import create_database, database_exists\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import MetaData\n",
    "import pymysql \n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import Table,Column,Integer, BIGINT, String, BigInteger, Boolean, Float, Unicode\n",
    "from sqlalchemy import ForeignKey, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.dialects.mysql import LONGTEXT\n",
    "meta = MetaData()\n",
    "# Database connection details\n",
    "user = 'root'\n",
    "password = 'hannibal1999' #your password here\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "db_name = 'iranketab'\n",
    "\n",
    "# Create the database if it doesn't exist\n",
    "# character set to utf8mb4 so that to_sql can work with persian text\n",
    "url = f'mysql://{user}:{password}@{host}:{port}/{db_name}?charset=utf8mb4'\n",
    "if not database_exists(url):\n",
    "    create_database(url)\n",
    "\n",
    "# Connect to the database\n",
    "engine = create_engine(url , pool_pre_ping=True)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Table(\n",
    "    'summary' , meta,\n",
    "    Column('url_id' , Integer , primary_key=True ),\n",
    "    Column('context' , Text),\n",
    "    mysql_charset='utf8mb4',\n",
    "    extend_existing=True,    \n",
    ") \n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = Table(\n",
    "    'book' , meta, \n",
    "    Column('id' , Integer , primary_key=True),\n",
    "    Column('url_id' , Integer , ForeignKey('summary.url_id') ),\n",
    "    Column('title' , String(1000)),\n",
    "    Column('en_title' , String(1000)),\n",
    "    Column('price' , BIGINT),\n",
    "    Column('ghat' , String(100)),\n",
    "    Column('discount' , Integer),\n",
    "    Column('ISBN' , String(100)),\n",
    "    Column('cover' , String(100)),\n",
    "    Column('page_count' , Integer),\n",
    "    Column('solar_publish_year' , Integer),\n",
    "    Column('ad_publish_year' , Integer),\n",
    "    Column('edition' , Integer),\n",
    "    Column('fastest_delivery' , String(15)),\n",
    "    Column('rate' , Float),\n",
    "    Column('available' , Boolean),      \n",
    "    mysql_charset='utf8mb4',\n",
    "    extend_existing=True,\n",
    ")\n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication = Table(\n",
    "    'publication' , meta,\n",
    "    Column('id' , Integer , primary_key=True),\n",
    "    Column('name' , String(100)),\n",
    "    extend_existing=True,\n",
    ") \n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_connector = Table(\n",
    "    'publication_connector' , meta, \n",
    "    Column('id' , Integer , primary_key=True , autoincrement=True),\n",
    "    Column('book_id' , Integer ,  ForeignKey('book.id') ),\n",
    "    Column('publication_id' , Integer ,  ForeignKey('publication.id') ),\n",
    "    extend_existing=True,\n",
    ")\n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = Table(\n",
    "    'tag' , meta,\n",
    "    Column('id' , Integer , primary_key=True ),\n",
    "    Column('name' , String(100)),\n",
    "    extend_existing=True,  \n",
    ") \n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_book = Table(\n",
    "    'tag_book' , meta, \n",
    "    Column('id' , Integer , primary_key=True , autoincrement=True),\n",
    "    Column('book_id' , Integer ,  ForeignKey('book.id') ),\n",
    "    Column('tag_id' , Integer ,  ForeignKey('tag.id') ),\n",
    "    extend_existing=True,\n",
    ")\n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = Table(\n",
    "    'person' , meta,\n",
    "    Column('id' , Integer , primary_key=True ),\n",
    "    Column('name' , String(100)),\n",
    "    extend_existing=True,\n",
    ") \n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = Table(\n",
    "    'author' , meta,\n",
    "    Column('id' , Integer , primary_key=True , autoincrement=True ),\n",
    "    Column('book_id' , Integer,  ForeignKey('book.id' ) ),\n",
    "    Column('person_id' , Integer  ,  ForeignKey('person.id')),\n",
    "    extend_existing=True,\n",
    ")\n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Table(\n",
    "    'translator' , meta,\n",
    "    Column('id' , Integer , primary_key=True , autoincrement=True),\n",
    "    Column('person_id' , Integer  , ForeignKey('person.id')),\n",
    "    Column('book_id' , Integer , ForeignKey('book.id') ),\n",
    "    extend_existing=True,\n",
    ")\n",
    "meta.create_all(engine)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: cleaning the data , inserting into database\n",
    "### summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from csv\n",
    "import pandas as pd\n",
    "df_summary = pd.read_csv ('book_summary.csv' , encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the extra \\n s in the text, fix column types and column names so they would be passed by to_sql()\n",
    "df_summary['text'] = df_summary['text'].str.strip().replace('\\n', '')\n",
    "df_summary = df_summary.astype({'book_id': 'int64'})\n",
    "df_summary.rename(columns={'book_id' : 'url_id'}, inplace=True)\n",
    "df_summary.rename(columns={'text' : 'context'}, inplace=True)\n",
    "df_summary = df_summary.astype({'context': 'string'})\n",
    "# fill nan values, pass data to csv\n",
    "df_summary['context'] = df_summary['context'].fillna('NaN')\n",
    "df_summary['context'] = df_summary['context'].str.replace('<NA>' , \"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99919"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the data to database\n",
    "df_summary.to_sql('summary' , con=engine ,  if_exists='append' , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### book table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parinaz\\AppData\\Local\\Temp\\ipykernel_13732\\1708306972.py:3: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_book = pd.DataFrame(pd.read_csv('book.csv' , encoding='UTF-8'))\n"
     ]
    }
   ],
   "source": [
    "# read data from csv\n",
    "import numpy as np\n",
    "df_book = pd.DataFrame(pd.read_csv('book.csv' , encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip unwanted \\n s from different columns, stripping the unwanted spaces, cleaning additional unwanted text\n",
    "df_book['title'] = df_book['title'].str.strip('\\n')\n",
    "df_book['ghat'] = df_book['ghat'].str.strip()\n",
    "df_book['ghat'] = df_book['ghat'].str.strip('\\n')\n",
    "df_book['price'] = df_book['price'].str.strip().replace('\\n' , '')\n",
    "df_book['ISBN'] = df_book['ISBN'].str.strip().replace('\\n' , '')\n",
    "df_book['cover'] = df_book['cover'].str.strip().replace('\\n' , '')\n",
    "df_book['cover'] = df_book['cover'].apply(lambda x: x.strip() if len(str(x)) > 5 else x)\n",
    "df_book['price'] = df_book['price'].str.replace(\",\" , \"\").fillna(0)\n",
    "df_book['edition'] = pd.to_numeric(df_book['edition'], errors='coerce').fillna(0.0)\n",
    "df_book['page_count'] = df_book['page_count'].str.strip().replace('\\n' , '').fillna('0')\n",
    "df_book['page_count'] = df_book['page_count'].apply(lambda x: '0' if not str(x).isdigit() else x)\n",
    "#df_book['page_count'] = df_book['page_count'].apply(lambda x: '0' if len(str(x)) > 5 else x)\n",
    "df_book['solar_publish_year'] = df_book['solar_publish_year'].str.strip().replace('\\n' , '')\n",
    "df_book['solar_publish_year'] = df_book['solar_publish_year'].apply(lambda x: str(x).zfill(4) if str(x).isdigit() and len(str(x)) == 4 else '0')\n",
    "df_book['ad_publish_year'] = df_book['ad_publish_year'].str.strip().replace('\\n' , '')\n",
    "df_book['ad_publish_year'] = df_book['ad_publish_year'].apply(lambda x: str(x).zfill(4) if str(x).isdigit() and len(str(x)) == 4 else '0')\n",
    "df_book['fastest_delivery'] = df_book['fastest_delivery'].str.strip('\\n')\n",
    "\n",
    "# Strip \"کتاب\" from titles\n",
    "df_book['title'] = df_book['title'].str.replace('کتاب', '')\n",
    "# Remove content within parentheses \n",
    "# سمفونی مردگان (جیبی)\n",
    "# to: سمفونی مردگان\n",
    "df_book['title'] = df_book['title'].str.replace(r\"\\(.*\\)\", '' , regex=True)\n",
    "df_book['title'] = df_book['title'].str.lstrip()\n",
    "df_book['title'] = df_book['title'].str.rstrip()\n",
    "\n",
    "\n",
    "# Replace NaN values in 'id' column with corresponding 'url_id' values\n",
    "booktemp = df_book.copy()\n",
    "booktemp['id'] = np.where(booktemp['id'].isnull() & ~booktemp['url_id'].isin(booktemp['id']), booktemp['url_id'], booktemp['id'])\n",
    "# Verify that there are no duplicate values in 'id' column\n",
    "booktemp.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "df_book = booktemp\n",
    "\n",
    "# adjusting column datatypes , fill nan values\n",
    "# booktemp = df_book[df_book['id'].isnull()]\n",
    "df_book.dropna(subset=['id'], inplace=True)\n",
    "df_book = df_book.astype({'id': 'int64'})\n",
    "df_book['discount'].fillna(0, inplace=True)\n",
    "df_book = df_book.astype({'discount': 'int64'})\n",
    "df_book['edition'].fillna(0, inplace=True)\n",
    "df_book = df_book.astype({'edition': 'int64'})\n",
    "df_book['solar_publish_year'].fillna(0, inplace=True)\n",
    "df_book = df_book.astype({'solar_publish_year': 'int64'})\n",
    "df_book['solar_publish_year'].fillna(0, inplace=True)\n",
    "df_book = df_book.astype({'ad_publish_year': 'int64'})\n",
    "df_book = df_book.astype({'available': 'bool'})\n",
    "df_book = df_book.astype({'title': 'str'})\n",
    "df_book = df_book.astype({'en_title': 'string'})\n",
    "df_book = df_book.astype({'price': 'int64'})\n",
    "df_book = df_book.astype({'ghat': 'string'})\n",
    "df_book = df_book.astype({'ISBN': 'string'})\n",
    "df_book = df_book.astype({'cover': 'string'})\n",
    "df_book = df_book.astype({'page_count': 'int64'})\n",
    "df_book = df_book.astype({'fastest_delivery': 'string'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fixing noise datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAGE COUNT MANUAL NOISE FIX\n",
    "df_book.loc[df_book['id'] == 88550, 'page_count'] = 758 #edited from amazon website\n",
    "df_book.loc[df_book['id'] == 95310, 'page_count'] = 680 #edited from other page counts on the internet\n",
    "df_book.loc[df_book['id'] == 87053, 'page_count'] = 298 #majale - edited from previous ones\n",
    "df_book.loc[df_book['id'] == 51430, 'page_count'] = 1724 #edited considering the price\n",
    "df_book.loc[df_book['id'] == 121685, 'page_count'] = 970 #edited from other websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113736"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing data to database\n",
    "df_book.to_sql('book' , con=engine , if_exists='append' , index=False  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### person , author and translator tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv\n",
    "import pandas as pd\n",
    "df_author = pd.read_csv('outher.csv' , encoding='UTF-8')\n",
    "df_translator = pd.read_csv('translator.csv' , encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the unwanted \\n s\n",
    "df_author['name'] = df_author['name'].str.strip('\\n')\n",
    "df_translator['name'] = df_translator['name'].str.strip('\\n')\n",
    "df_translator['name'] = df_translator['name'].str.replace('\\n' , '')\n",
    "\n",
    "# reading name list from df s, removing duplicates and make df_person\n",
    "namelist = pd.DataFrame(df_translator['name'])\n",
    "name_list = pd.DataFrame(df_author['name'])\n",
    "names = pd.concat([namelist, name_list], ignore_index=True)\n",
    "names.drop_duplicates(subset=['name'] , inplace=True )\n",
    "names = names.assign(id=range(1, len(names) + 1))\n",
    "names.reset_index(inplace=True , drop=True)\n",
    "df_person = names\n",
    "df_person.dropna(subset=['name'], inplace=True)\n",
    "#df_translator.dropna(subset=['name'], inplace=True)\n",
    "#df_author.dropna(subset=['name'], inplace=True)\n",
    "\n",
    "# id_book column name from csv changed to pass to database\n",
    "df_translator.rename(columns={'id_book' : 'book_id'}, inplace=True)\n",
    "df_author.rename(columns={'id_book' : 'book_id'}, inplace=True)\n",
    "# create relation with merge\n",
    "df_author = df_author.merge(df_person[['name', 'id']], on='name', how='left')\n",
    "df_author = df_author.rename(columns={'id': 'person_id'})\n",
    "\n",
    "df_translator = df_translator.merge(df_person[['name', 'id']], on='name', how='left')\n",
    "df_translator = df_translator.rename(columns={'id': 'person_id'})\n",
    "# clean data, make sure there's no duplicates\n",
    "df_author.drop_duplicates(subset=['person_id'] , inplace=True )\n",
    "df_author.drop('name' , axis=1 , inplace=True)\n",
    "#df_author.drop_duplicates(subset=['book_id'] , inplace=True )\n",
    "df_translator.drop('name' , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54767"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the data to database\n",
    "df_person.to_sql('person' , con=engine ,  if_exists='append' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40388"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the data to database\n",
    "df_author.to_sql('author' , con=engine ,  if_exists='append' , index=False)\n",
    "# pass the data to database\n",
    "df_translator.to_sql('translator' , con=engine ,  if_exists='append' , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### publish_connector and publication tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv\n",
    "df_pub_connector = pd.read_csv('publisher.csv' , encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column from csv to pass data to database, strip unwanted \\n s\n",
    "df_pub_connector['name'] = df_pub_connector['name'].str.strip('\\n')\n",
    "df_pub_connector = df_pub_connector.rename(columns={'id_book': 'book_id'})\n",
    "# create publication df, make sure there's no duplicates\n",
    "df_publication = pd.DataFrame(df_pub_connector['name'])\n",
    "df_publication.drop_duplicates(subset=['name'] , inplace=True )\n",
    "df_publication = df_publication.assign(id=range(1, len(df_publication) + 1))\n",
    "df_publication.reset_index(inplace=True , drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make publication connector with merge\n",
    "df_pub_connector = df_pub_connector.merge(df_publication[['name' , 'id']] , on='name' , how='left')\n",
    "df_pub_connector = df_pub_connector.rename(columns={'id' : 'publication_id'})\n",
    "df_pub_connector.drop('name' , axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the data to database\n",
    "df_publication.to_sql('publication' , con=engine ,  if_exists='append' , index=False)\n",
    "df_pub_connector.to_sql('publication_connector' , con=engine ,  if_exists='append' , index=False)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag and tag_book tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Read the tag and award CSV files\n",
    "df_tag = pd.read_csv('tag.csv', encoding='UTF-8')\n",
    "df_tag.drop('tag_id', axis=1, inplace=True)\n",
    "\n",
    "df_tag['tag'] = df_tag['tag'].apply(eval)\n",
    "df_tag['tag'] = df_tag['tag'].apply(lambda x: [i.strip() for i in x])\n",
    "\n",
    "df_tag= df_tag.explode('tag')\n",
    "df_tag_book = df_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag table creation\n",
    "tags = pd.DataFrame(df_tag['tag'])\n",
    "# we just want unique names for tags\n",
    "tags.drop_duplicates(subset=['tag'] , inplace=True)\n",
    "# creating id column for tags -> tag.id / tag_id\n",
    "tags = tags.assign(id=range(1, len(tags) + 1))\n",
    "tags.reset_index(inplace=True , drop=True)\n",
    "\n",
    "# drop nan s in both tables\n",
    "df_tag_book.dropna(subset=['tag'], inplace=True)\n",
    "tags.dropna(subset=['tag'], inplace=True)\n",
    "\n",
    "# rename columns , no name column needed in tag_book\n",
    "df_tag_book = df_tag_book.merge(tags[['tag' , 'id']] ,  on='tag', how='left')\n",
    "tags.rename(columns={'tag' : 'name'}, inplace=True)\n",
    "df_tag_book.rename(columns={'id' : 'tag_id'}, inplace=True)\n",
    "df_tag_book.drop('tag' , axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519226"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.to_sql('tag', con=engine, if_exists='append', index=False)\n",
    "df_tag_book.to_sql('tag_book', con=engine, if_exists='append', index=False)\n",
    "session.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
